# # View hdf5 files stored in pyUSID format
#
# Learn about the ways we can open and use hdf5 files generated by this project
#
# > This notebook is part of an experimental branch. The exact syntax of the
# attribute access may change.
#
# ## File structure
#
# Every hdf5 file is structured in pyUSID format. Full compatibility with the
# [pycroscopy][pycroscopy] ecosystem is available, but this notebook aims to
# highlight how one could use default extraction methods to inspect the
# contents of a single hdf5 file.
#
# [pycroscopy]: https://github.com/pycroscopy/pycroscopy
#
# The pyUSID format exects that the main meausurment of an instrument
# acquisition process is saved as a 2D array and is identifiable with some
# mandatory metadata describing the acquisition.
#
# In the hdf5 files generated by this project, measurements live under a group
# called `Measurement_XXX` where `XXX` is a sequentially increasing index
# starting from `000`. In this group, there is a main dataset called `Raw_Data`
# structured as a 2D array.
#
# The ancillary datasets `Position_Indices` and `Position_Values` together
# describe a mapping that allows the association of each index in the 2D
# `Raw_Data` with physical coordinates. These coordinates may be expressed in
# real units (i.e. `m`) or in instrument units (i.e. `pixel`).
#
# The ancillary datasets `Spectroscopic_Indices` and `Spectroscopic_Values`
# together describe a mapping that allows the association of each array in the
# 2D `Raw_Data` with appropriate labels. This mapping may represent a
# wavelength association with each CCD pixel or a specific channel for (R,G,B)
# image data.

# +
import pathlib
import sys

import h5py

import matplotlib.pyplot as plt
import numpy as np
import pyUSID as usid


# This project saves data in a network drive, available only from a Vanderbilt
# IP address.
# If you connect your personal computer to this network drive, you
# can access its files
if "darwin" in sys.platform:
    NETWORK_PATH = "/Volumes/public/"
elif "win32" in sys.platform:
    NETWORK_PATH = (
        "Z://"  # Adjust as necessary, depending on how you mounted network drive
    )
NETWORK_DIR = pathlib.Path(NETWORK_PATH)

# By default, search the data directory from the network path above.
# In the event the network drive is unavailable, assume local data is saved
# with this project in a folder named `data`, available from the project root
# (or one directory behind this notebook).
if NETWORK_DIR.exists():
    print(
        "Connected to Network Directory. Defaulting search to the following location:"
    )
    DATA_DIR = pathlib.Path(NETWORK_DIR, "David Curie/Data")
else:
    print("Network Directory unavailable. Defaulting search to the following location:")
    DATA_DIR = pathlib.Path("/Users/anishgiri/Documents/Haglund_stevenson/Witec/data/hBN-10-CH4")
print(DATA_DIR)

# +
# Find a specific file by search patterns
search_pattern = "*.hdf5"

# List matching files
h5_files = sorted(DATA_DIR.glob(search_pattern))
for i, file in enumerate(h5_files):
    print(i, file.name)

# +
# Select a file according to desired index above
h5_path = list(h5_files)[12]

# Load the file manually
with h5py.File(h5_path, mode="r") as h5_file:
    # Print a directory tree
    for key in h5_file.keys():
        print(key)
        for dataname in h5_file[key].keys():
            print("   ", dataname)
# -

# The below snippet shows how you might access datasets within a single hdf5 file.
#
# Group access mimics typical dictionary access for Python dictionaries, but
# remain compressed in memory for typical access. This means the dictionary
# values are lost when you close the file.

# +
# View the Raw Data as a 2D array
with h5py.File(h5_path, mode="r") as h5_file:
    # Extract datasets
    temporary_main_dataset = h5_file["Measurement_000"]["Raw_Data"]
    temporary_position_dataset = h5_file["Measurement_000"]["Position_Values"]
    temporary_spectroscopic_dataset = h5_file["Measurement_000"]["Spectroscopic_Values"]

    # Show that extrated datasets are still bundled
    print("DISPLAYING DATASETS AS PYTHON SEES THEM IN MEMORY\n")
    print(temporary_main_dataset)
    print(temporary_position_dataset)
    print(temporary_spectroscopic_dataset)

    # Access the values of the datasets with [()] notation
    print("\n-------------------------------")
    print("DISPLAYING CONTENTS OF DATASETS\n")
    print("Main dataset:\n", temporary_main_dataset[()])
    print("Position dataset:\n", temporary_position_dataset[()])
    print("Spectroscopic dataset:\n", temporary_spectroscopic_dataset[()])

print("\n---------------------------------------------")
print("DISPLAYING DATASETS AFTER THE FILE IS CLOSED\n")
print("Main dataset:", temporary_main_dataset)
print("Position dataset:", temporary_position_dataset)
print("Spectroscopic dataset:", temporary_spectroscopic_dataset)
# -

# If you need continued access to the contents of a single hdf5 file after the
# file is closed, write the contents to another variable as a copy.

# +
# View the Raw Data as a 2D array
with h5py.File(h5_path, mode="r") as h5_file:
    # Extract datasets
    temporary_main_dataset = h5_file["Measurement_000"]["Raw_Data"]
    temporary_position_dataset = h5_file["Measurement_000"]["Position_Values"]
    temporary_spectroscopic_dataset = h5_file["Measurement_000"]["Spectroscopic_Values"]

    # Show that extrated datasets are still bundled
    main_dataset = temporary_main_dataset[()].copy()
    position_dataset = temporary_position_dataset[()].copy()
    spectroscopic_dataset = temporary_spectroscopic_dataset[()].copy()

print("Main dataset:", main_dataset.shape, "\n", main_dataset, "\n")
print("Position dataset:", position_dataset.shape, "\n", position_dataset, "\n")
print(
    "Spectroscopic dataset:",
    spectroscopic_dataset.shape,
    "\n",
    spectroscopic_dataset,
    "\n",
)
# -

# As an example of how you can use this data to create custom visualizations in
# native Python syntax, consider the following.
#
# The cell below will create a 2D image representing the intensity values
# gathered at each coordinate of an `External Spectrum` acquired from
# _ScanCtrlSpectroscopyPlus_. The 2D main dataset is converted to a 3D array
# whose shape is determined by the number of unique x and y coordinates from
# the position dataset.




# +
rows = np.unique(position_dataset[:, 0])  # x coordinates
cols = np.unique(position_dataset[:, 1])  # y coordinates
position_values = np.reshape(position_dataset, (len(rows), len(cols), -1))

intensity_values = np.sum(
    main_dataset, axis=1
)  # Sum intensity across all pixels in each spectrum

# intensity_values = []

# for i in range(len(main_dataset)):
#     a = three_maxima_points(spectroscopic_dataset[0], main_dataset[i])
#     intensity_values.append(a[1][1])
    

intensity_map = np.reshape(
    intensity_values, (len(rows), len(cols), -1)
)  # shape (x, y, I) where I is Intensity



fig, ax = plt.subplots()
cax = ax.imshow(intensity_map, cmap="gist_stern")

# Add colorbar
cbar = plt.colorbar(cax)
cbar.set_label('Intensity', rotation=270, labelpad=15)

plt.show()
# -


# # pyUSID native exploration
#
# Because the files are saved in pyUSID format, you can make use of some of
# pyUSID's helper scripts.
#
# > Make sure you have pyUSID installed in your conda environment (`conda
# install -c conda-forge pyusid`). This dependency is not yet commited to the
# main repository, so if you followed instructions to install your dependencies
# as outlined in the `main` Git branch, and then switched branches to
# `feature/hdf5` to run this notebook, you won't have the updated dependency.
# Once you install this dependency once, you won't need to install it again
# when switching branches.

# +
import numpy as np
from scipy.ndimage import gaussian_filter
from scipy.signal import find_peaks
import matplotlib.pyplot as plt

def smooth(list_x, list_y):
    # Assuming list_x and list_y are your data points
    list_x = np.array(list_x)  # Make sure list_x is a NumPy array
    list_y = np.array(list_y)
    
    # Applying a Gaussian filter to smooth the data
    smoothed_y = gaussian_filter(list_y, sigma=1.7)
    
    # Define the height threshold
    height_threshold = max(smoothed_y) / 2
    
    # Re-running the peak finding with smoothed data
    smoothed_peaks, _ = find_peaks(smoothed_y, height=height_threshold)
    
    # Plotting the smoothed data and the detected peaks
    plt.figure(figsize=(12, 6))
    #plt.scatter(x_data, y_data, label='Data', s=1)
    
    #plt.plot(list_x, smoothed_y, label='Smoothed Data')
    #plt.plot(list_x[smoothed_peaks], smoothed_y[smoothed_peaks], "x", label='Detected Peaks')
    plt.title('Smoothed Data')
    plt.xlabel('X')
    plt.ylabel('Y')
    plt.legend()
    plt.show()
    return [list_x, smoothed_y]
    
list_1 = spectroscopic_dataset[0]
list_2 = main_dataset[550]
data_spec = smooth(list_1, list_2)     


# +

def plot_fit(wavelength_list, amplitude_list):
    import numpy as np
    from scipy.optimize import curve_fit
    import matplotlib.pyplot as plt
    import math
    
    import pandas as pd  
    
    def lorentz(x, amp, wid, cen):
        return amp * (1/np.pi) * (wid / ((x - cen)**2 + wid**2))
    
    def gaussian(x, amp, wid, cen):
        return amp * np.exp(-((x - cen)**2) / (2*wid**2))
    
    def function_mix(x, params, number):
        total_function = np.zeros_like(x)
        for i in range(number):
            amp = params[i*3]
            cen = params[i*3 + 1]
            wid = params[i*3 + 2]
            total_function += gaussian(x, amp, wid, cen)
        return total_function + params[-1]
        
    # Sample dataset
    list_1 = wavelength_list
    list_2 = amplitude_list
    data_spec = smooth(list_1, list_2)   
    
    list_1 = data_spec[0]
    list_2 = data_spec[1]
    
    list_x = []
    list_y = []
    
    for i in range(len(list_1)):
        if (list_1[i] > 480 and list_1[i] < 700):
            list_x.append(list_1[i])
            list_y.append(list_2[i])
    
    # Convert lists to NumPy arrays
    x_data = np.array(list_x)
    y_data = np.array(list_y)
    
    initial_guess = [2000, 537, 0.5,
                     3000, 543, 0.5,
                     3000, 543.26, 4.40,
                     2100, 549, 1,
                     1800, 556.13, 0.68,
                     400, 560, 0.3,
                     800, 564, 0.48,
                     800] # closest 532, 542, gives more appropriate but one negative
    
    upper_bounds = [2000+200, 537+1, 0.5+0.3,
                    3000+500, 543+2, 0.5+0.3,
                    3000+200, 543.26+1, 4.40+2,
                    2100+100, 549+1, 1+0.5,
                    1800+100, 556.13+1, 0.68+0.2,
                    400+100, 560+1, 0.3+1,
                    800+100, 564+1, 0.48+0.3,
                    800+100]
    
    lower_bounds = [2000-200, 537-1, 0.5-0.3,
                    3000-500, 543-2, 0.5-0.3,
                    3000-200, 543.26-1, 4.40-2,
                    2100-100, 549-1, 1-0.5,
                    1800-100, 556.13-1, 0.68-0.2,
                    400-100, 560-1, 0.3-1,
                    800-100, 564-1, 0.48-0.3,
                    800-50]
    
    
    Num_functions = 6
    params, covariance = curve_fit(
        lambda x, *params: function_mix(x, params, Num_functions), 
        x_data, 
        y_data, 
        p0=initial_guess,
        bounds=(lower_bounds, upper_bounds),
        maxfev=1000000
    )
    
    for i in range(Num_functions):
        amp = params[i*3]
        cen = params[i*3 + 1]
        wid = params[i*3 + 2]
        
        y_fun_individual = lorentz(x_data, amp, wid, cen)
        #plt.plot(x_data, y_fun_individual, label=f'L {i+1}', linestyle='--', linewidth=1)
        
    y_fitted = function_mix(x_data, params, Num_functions)
    #plt.figure()
    #plt.plot(x_data, y_fitted, label='Fitted function with all Gaussians', linewidth=1)
    #plt.plot(list_x, list_y, label='Smoothed Data')

    ###

    # Extracting central wavelength values from params
    central_wavelengths = [params[i*3 + 1] for i in range(Num_functions)]
    
    # Sorting the central wavelengths and keeping track of the indices
    sorted_indices = np.argsort(central_wavelengths)
    sorted_central_wavelengths = np.array(central_wavelengths)[sorted_indices]
    
    # Calculating y-values for sorted central wavelengths
    sorted_y_values = [function_mix(np.array([cw]), params, Num_functions)[0] for cw in sorted_central_wavelengths]
    
    # Combining x and y values
    sorted_peaks = list(zip(sorted_central_wavelengths, sorted_y_values))

    return sorted_peaks

list_1 = spectroscopic_dataset[0]
list_2 = main_dataset[550]
plotting = plot_fit(list_1, list_2)


# +
def colorplot_flake(num):
    rows = np.unique(position_dataset[:, 0])  # x coordinates
    cols = np.unique(position_dataset[:, 1])  # y coordinates
    position_values = np.reshape(position_dataset, (len(rows), len(cols), -1))
    
    intensity_values = np.sum(
        main_dataset, axis=1
    )  # Sum intensity across all pixels in each spectrum
    
    intensity_values = []
    
    for i in range(len(main_dataset)):
        a = plot_fit(spectroscopic_dataset[0], main_dataset[i])
        intensity_values.append(a[num][1])
        
    
    intensity_map = np.reshape(
        intensity_values, (len(rows), len(cols), -1)
    )  # shape (x, y, I) where I is Intensity
    
    
    
    fig, ax = plt.subplots()
    cax = ax.imshow(intensity_map, cmap="gist_stern")
    
    # Add colorbar
    cbar = plt.colorbar(cax)
    wav = 111
    bar.set_label(f'Intensity at {wav} nm', rotation=270, labelpad=15)
    
    plt.show()

colorplot_flake(4)
# -


